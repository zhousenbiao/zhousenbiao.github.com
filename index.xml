<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>周森标博客</title>
    <link>http://senbiao.org/</link>
    <description>Recent content on 周森标博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Mar 2018 09:47:01 +0800</lastBuildDate>
    <atom:link href="http://senbiao.org/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>利用CNN做图像压缩，TNG学习笔记</title>
      <link>http://senbiao.org/post/tng-cnn/</link>
      <pubDate>Fri, 30 Mar 2018 09:47:01 +0800</pubDate>
      
      <guid>http://senbiao.org/post/tng-cnn/</guid>
      <description>TNG(Tiny Network Graphics)，一种图像压缩技术，功能是将低分辨图片转化为高清版本，旨在保持图片的质量下，尽可能降低图片的大小，使用户在带宽受限的网络情况下，仍可看到高清的图像 TNG 采用的算法是卷积神经网络（CNN）。卷积神经网络是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围，适合大型图像处理。 一个卷积神经网络由卷积、池化、非线性函数、归一化层等模块组成。最终的输出根据应用而定，如在人脸识别领域，我们可以用它来提取一串特征来表示一幅人脸图片。然后通过比较特征的异同进行人脸识别。 TNG采用了量化与反量化的技术。通过量化技术将浮点数转换为整数或二进制数，这时通常采用的方法是：去除浮点数后面的小数，将浮点数变成整数。在解码端，又采用反量化技术将变换后的特征数据恢复成浮点数，如给整数加上一个随机小数。这样可以一定程度上降低量化对神经网络精度的影响，从而提高恢复图像的质量。 如何利用卷积神经网络做压缩？ 以下两张图片来自不同的文章，总体意思差不多，只有一些细节上的差异： 如图1（用深度学习进行图片压缩示意图）所示，【完整的深度学习图片压缩框架】包括CNN编码器、量化、反量化、CNN解码器、熵编码、码字估计和码率-失真优化等几个模块。编码器的作用是将图片转换为压缩特征，解码器就是从压缩特征恢复出原始图片。其中编码网络和解码器，可以用卷积、池化、非线性等模块进行设计和搭建。 如图 2（用深度学习进行图片压缩示意图）所示，完整的框架包括 CNN 编码网络、量化、反量化、CNN 解码、熵编码等几个模块。编码网络的作用是将图片转换为压缩特征，解码网络就是从压缩特征恢复出原始图片。其中编码网络和解码网络，可以用卷积、池化、非线性等模块进行设计和搭建。 如何评判压缩算法 在深入技术细节前，我们先来了解一下如何评判压缩算法。评判一个压缩算法好坏的重要指标有两个：一个是每个像素占据的比特位数（bit per pixel，BPP），一个是 PSNR。我们知道，数据在计算机中以比特形式存储，所需比特数越多则占据的存储空间越大。BPP 用于表示图像中每个像素所占据的比特数，如一张 RGB 三通道图，表示每个像素需要消耗 24 个比特。PSNR 用来评估解码后图像的恢复质量，简单理解就是 PSNR 越高，恢复质量越好。 我们举个例子，假设长宽为 768*512 的图片大小为 1M，利用深度学习技术对它编码，通过编码网络后产生包括 96*64*192 个数据单元的压缩特征数据，如果表示每个数据单元平均需要消耗 1 个比特，则编码整张图需要 96*64*192 个比特。经过压缩后，编码每个像素需要的比特数为（96*64*192）/(768*512）=3，所以 BPP 值为 3bit/pixel，压缩比为 24:3=8:1。这意味着一张 1M 的图，通过压缩后只需要消耗 0.125M 的空间，换句话说，之前只能放 1 张照片的空间，现在可以放 8 张。 如何用深度学习做压缩 谈到如何用深度学习做压缩，还是用刚才那个例子。将一张大小 768*512 的三通道图片送入编码网络，进行前向处理后，会得到占据 96*64*192 个数据单元的压缩特征。有计算机基础的读者可能会想到，这个数据单元中可放一个浮点数，整形数，或者是二进制数。那问题来了，到底应该放入什么类型的数据？从图像恢复角度和神经网络原理来讲，如果压缩特征数据都是浮点数，恢复图像质量是最高的。但一个浮点数占据 32 个比特位，那之前讲的比特数计算公式变为（96*64*192*32）/（768*512）=96，压缩后反而每个像素占据比特从 24 变到 96，非但没有压缩，反而增加了，这是一个糟糕的结果，很显然浮点数不是好的选择。 所以为了设计靠谱的算法，我们使用一种称为量化的技术，它的目的是将浮点数转换为整数或二进制数，最简单的操作是去掉浮点数后面的小数，浮点数变成整数后只占据 8 比特，则表示每个像素要占据 24 个比特位。与之对应，在解码端，可以使用反量化技术将变换后的特征数据恢复成浮点数，如给整数加上一个随机小数，这样可以一定程度上降低量化对神经网络精度的影响，从而提高恢复图像的质量。 即使压缩特征中每个数据占据 1 个比特位，可是 8:1</description>
    </item>
    
    <item>
      <title>常见排序算法分类</title>
      <link>http://senbiao.org/post/algorithms/</link>
      <pubDate>Wed, 21 Mar 2018 14:01:42 +0800</pubDate>
      
      <guid>http://senbiao.org/post/algorithms/</guid>
      <description>常见排序算法一般分为以下几种： 非线性时间比较类排序：交换类排序（快速排序和冒泡排序）、插入类排序（简单插入排序和希尔排序）、选择类排序（简单选择排序和堆排序）、归并排序（二路归并排序和多路归并排序）； 线性时间非比较类排序：计数排序、基数排序和桶排序。 总结： 在比较类排序中，归并排序最快，其次是快速排序和堆排序，两者不相伯仲，但是有一点需要注意，数据初始排序状态对堆排序不会产生太大的影响，而快速排序却恰恰相反。 线性时间非比较类排序一般要优于非线性时间比较类排序，但前者对待排序元素的要求较为严格，比如计数排序要求待排序数的最大值不能太大，桶排序要求元素按照hash分桶后桶内元素的数量要均匀。线性时间非比较类排序的典型特点是以空间换时间。</description>
    </item>
    
    <item>
      <title>机器学习涉及的数学知识</title>
      <link>http://senbiao.org/post/math-ml/</link>
      <pubDate>Mon, 19 Mar 2018 15:17:26 +0800</pubDate>
      
      <guid>http://senbiao.org/post/math-ml/</guid>
      <description>在过去几个月里，有几个人联系过我，说他们渴望进军数据科学领域，使用机器学习 (ML) 技术探索统计规律，并打造数据驱动的完美产品。但是，据我观察，一些人缺乏必要的数学直觉和框架，无法获得有用的结果。这是我决定写这篇博客文章的主要原因。最近，易用的机器学习和深度学习工具包急剧增加，比如scikit-learn、Weka、Tensorflow、R-caret等。机器学习理论是一个涵盖统计、概率、计算机科学和算法方面的领域，该理论的初衷是以迭代方式从数据中学习，找到可用于构建智能应用程序的隐藏洞察。尽管机器学习和深度学习有巨大的发展潜力，但要深入掌握算法的内部工作原理并获得良好的结果，就必须透彻地了解许多技术的数学原理。 为什么担忧数学？ 出于许多原因，机器学习的数学原理很重要，下面重点介绍部分原因： 选择正确的算法，这涉及到考虑准确率、训练时间、模型复杂性、参数数量和特征数量。 选择参数设置和验证策略。 通过理解偏差-方差权衡，识别欠拟合和过拟合。 估算正确的置信区间和不确定性。 您需要多高的数学知识水平？ 在尝试理解诸如机器学习这样的跨学科领域时，需要考虑的主要问题是，理解这些技术需要多大的数学知识量和多高的数学知识水平。此问题的答案涉及多个维度，而且取决于个人的知识水平和兴趣。对机器学习的数学公式和理论发展的研究从未间断过，一些研究人员正在研究更高级的技术。我将介绍我认为成为机器学习科学家/工程师所需的最低数学知识水平，以及每个数学概念的重要性。 线性代数：同事 Skyler Speakman 最近说“线性代数是 21 世纪的数学”，我完全同意这种说法。在机器学习中，线性代数无处不在。要理解用于机器学习的优化方法，需要掌握许多主题，比如主成份分析 (PCA)、奇异值分解 (SVD)、矩阵特征分解、LU 分解、QR 分解/因式分解、对称矩阵、正交化/标准正交化、矩阵运算、投影、特征值和特征矢量、矢量空间，以及范数。关于线性代数，令人惊奇的是网上有如此多的资源。我总是说，由于互联网上存在大量资源，传统的课堂教学正在消亡。我最喜欢 MIT Courseware（Gilbert Strang 教授）提供的线性代数课。 概率论和统计学：机器学习与统计学并不是完全不同的领域。实际上，有人最近将机器学习定义为‘在 Mac 上实践统计学’。机器学习需要的一些基本的统计和概率理论包括组合学、概率规则和公理、贝叶斯定理、随机变量、方差和预期、条件和联合分布、标准分布（伯努利、二项式、多项式、均匀和高斯分布）、矩母函数、最大似然估计 (MLE)、先验和后验、最大后验概率估计 (MAP)，以及采样方法。 多变量微积分：一些必要的主题包括微积分、偏微分、矢量-值函数、方向梯度、海赛函数、雅可比行列式、拉普拉斯算子和拉格朗日分布。 算法和复杂优化：这对理解机器学习算法的计算效率和可伸缩性，以及利用数据集的稀疏性都很重要。需要数据结构（二叉树、哈希运算、堆、堆栈等）、动态编程、随机化和次线性算法、图表、梯度/随机下降，以及原对偶方法的知识。 其他：包括上述 4 个主要领域未涵盖的其他数学主题。这些主题包括实数和复数分析（集合和数列、拓扑、度量空间、单值和连续函数、极限、柯西核、傅里叶变换），信息论（熵、信息增益），函数空间和数集。 本文摘自：这里.</description>
    </item>
    
    <item>
      <title>K-近邻算法学习笔记</title>
      <link>http://senbiao.org/post/k-nearest-neighbors/</link>
      <pubDate>Mon, 19 Mar 2018 14:38:50 +0800</pubDate>
      
      <guid>http://senbiao.org/post/k-nearest-neighbors/</guid>
      <description>k-近邻算法，目的就是找到新数据的前k个邻居，然后根据邻居的分类来确定该数据的分类。 所谓邻居，就是距离近的。因此需要一个“距离度量”，以2个特征的样本为例，也就是在2维实数向量空间，可以使用两点距离公式计算距离： $$ \sqrt{(x1-x2)^2+(y1-y2)^2} $$ k-近邻算法步骤如下： 计算已知类别数据集中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的k个点； 确定前k个点所在类别的出现频率； 返回前k个点所出现频率最高的类别作为当前点的预测分类。 比如，现在我这个k值取3，那么在电影例子中，按距离依次排序的三个点分别是动作片(108,5)、动作片(115,8)、爱情片(5,89)。 在这三个点中，动作片出现的频率为三分之二，爱情片出现的频率为三分之一，所以该红色圆点标记的电影为动作片。 这个判别过程就是k-近邻算法。 from：http://cuijiahua.com/blog/2017/11/ml_1_knn.html</description>
    </item>
    
    <item>
      <title>利用Tensorflow和matplotlib直观理解CNN的卷积层与池化层</title>
      <link>http://senbiao.org/post/tensorflow-cnn-pool-convolutional/</link>
      <pubDate>Tue, 16 Jan 2018 21:43:25 +0800</pubDate>
      
      <guid>http://senbiao.org/post/tensorflow-cnn-pool-convolutional/</guid>
      <description>卷积神经网络，CNN（Convolutional Neural Network），卷积神经网络是一种多层神经网络，擅长处理图像相关的深度学习问题。 与普通神经网络的区别在于，卷积神经网络包含了由卷积层（Convolutional layer）和池化层（Pooling layer）构成的特征抽取器。 本文旨在通过调用tensorflow封装好的卷积操作和池化操作函数以及python的matplotlib库，直观地感受一张图片经过卷积层和池化层之后的效果，代码如下： #!/usr/bin/python -- coding: UTF-8 -- import matplotlib.pyplot as plt import tensorflow as tf from PIL import Image import numpy img = Image.open(&amp;lsquo;dog.jpg&amp;rsquo;) img_ndarray = numpy.asarray(img, dtype=&amp;lsquo;float32&amp;rsquo;) print(img_ndarray.shape) img_ndarray=img_ndarray[:,:,0] plt.figure() plt.subplot(221) plt.imshow(img_ndarray) w=[[-1.0,-1.0,-1.0], [-1.0,9.0,-1.0], [-1.0,-1.0,-1.0]] with tf.Session() as sess: img_ndarray=tf.reshape(img_ndarray,[1, 633, 650,1]) w=tf.reshape(w,[3,3,1,1]) img_cov=tf.nn.conv2d(img_ndarray, w, strides=[1, 1, 1, 1], padding=&amp;lsquo;SAME&amp;rsquo;) image_data=sess.run(img_cov) print(image_data.shape) plt.subplot(222) plt.imshow(image_data[0,:,:,0]) img_pool=tf.nn.max_pool(img_ndarray, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)</description>
    </item>
    
    <item>
      <title>利用Tensorflow构建CNN，步骤梳理</title>
      <link>http://senbiao.org/post/tensorflow-cnn-mnist/</link>
      <pubDate>Fri, 08 Dec 2017 13:04:48 +0800</pubDate>
      
      <guid>http://senbiao.org/post/tensorflow-cnn-mnist/</guid>
      <description>利用Tensorflow构建CNN卷积神经网络，以mnist手写识别为例，步骤梳理如下： 1.定义权重函数 def weight_variable(shape): # 使用正态分布填充变量，标准差设置为0.1 initial = tf.truncated_normal(shape, stddev=0.1) #返回相应的变量 return tf.Variable(initial) 2.定义偏置函数 def bias_Variable(shape): # 定义一个常量该常量全部由0.1组成，shape由参数shape指定 initial = tf.constant(0.1, shape=shape) # 返回一个变量，变量的值由刚刚定义的常量决定 return tf.Variable(initial) 3.定义卷积层 def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) 4.定义池化层 def max_pool(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) 5.定义CNN网络结构 # 构建网络，由2个卷积层（包含激活层、池化层），一个全连接层，一个dropout层，一个softmax层 组成 def deepnn(x): x_image = tf.reshape(x, [-1, 28, 28, 1]) W_conv1 = weight_variable([5, 5, 1,</description>
    </item>
    
  </channel>
</rss>
