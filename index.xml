<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>周森标博客 on 周森标博客</title>
    <link>http://senbiao.org/</link>
    <description>Recent content in 周森标博客 on 周森标博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Aug 2018 15:04:05 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>tensorflow版本问题，导致 &#39;DecodeJpeg/contents&#39;, does not exist in the graph.</title>
      <link>http://senbiao.org/post/tensorflow-version-error/</link>
      <pubDate>Thu, 02 Aug 2018 15:04:05 +0800</pubDate>
      
      <guid>http://senbiao.org/post/tensorflow-version-error/</guid>
      <description>从tensorflow的github克隆下来后，git checkout r1.6 然后在tensorflow目录下跑flower_photos数据集
训练命令如下：
python examples/image_retraining/retrain.py --learning_rate=0.0001 --testing_percentage=20 --validation_percentage=20 --train_batch_size=32 --validation_batch_size=-1 --flip_left_right True --random_scale=30 --random_brightness=30 --eval_step_interval=100 --how_many_training_steps=600 --architecture mobilenet_1.0_224 --image_dir ~/Documents/yolo-darkne/flower_photos  训练结束后，在/tmp/imagenet目录下得到模型文件和对应标签文件：output_graph.pb、output_labels.txt
用如下代码调用模型：
# -*- coding:utf-8 -*- import tensorflow as tf import sys # 命令行参数，传入要判断的图片路径 image_file = sys.argv[1] #print(image_file) # 读取图像 image = tf.gfile.FastGFile(image_file, &#39;rb&#39;).read() # 加载图像分类标签 labels = [] for label in tf.gfile.GFile(&amp;quot;/tmp/imagenet/output_labels.txt&amp;quot;): labels.append(label.rstrip()) # 加载Graph with tf.gfile.FastGFile(&amp;quot;/tmp/imagenet/output_graph.pb&amp;quot;, &#39;rb&#39;) as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) tf.import_graph_def(graph_def, name=&#39;&#39;) with tf.</description>
    </item>
    
    <item>
      <title>深度计数:基于深度模拟学习的果实计数</title>
      <link>http://senbiao.org/post/deep-simulated-learning-fruit-counting/</link>
      <pubDate>Wed, 01 Aug 2018 18:00:29 +0800</pubDate>
      
      <guid>http://senbiao.org/post/deep-simulated-learning-fruit-counting/</guid>
      <description>原文：Deep Count: Fruit Counting Based on Deep Simulated Learning
摘要 近年来，基于深度学习的计算机视觉研究取得了重大进展。这些任务的成功很大程度上依赖于大量的训练样本。对训练样本进行标记是一个挺费劲的过程。在文中，我们提出了一种基于模拟深度卷积神经网络的产量估算的方法。了解果树、花卉和树木的确切数量有助于农民更好地做出关于栽培实践、植物病害预防和收获劳动力规模的决定。目前人工估算果实或花卉的产量估算方法是一个费时费力的过程，在大面积的生产中是不实用的。基于机器人农业的自动产量估计在这方面提供了可行的解决方案。该网络由合成数据进行训练，并在真实数据上进行测试。为了在多个尺度上捕获特征，我们使用了一个改进版本的Inception-RESNET架构。即使在阴影下、叶子、树枝遮挡，或有一定程度的重叠，我们的算法计数依然表现良好。实验结果表明，实际图像的平均测试精度为91%，合成图像的测试精度为93%。
1. 简介 近年来，基于深度学习的计算机视觉研究取得了巨大的进步。各种基于视觉的任务，如物体识别、分类和计数，可以实现高精度。这些高级任务的成功很大程度上依赖于大量的训练样本。训练样本的标注是一个费时费力的过程。而生成用于训练的合成数据则提供另一种解决的方案。本研究的目的之一是通过创建用于训练的合成数据集来减少对对象计数问题的训练样本的标记开销。本研究实现了一种基于深度模拟学习的快速产量估计的方法。准确的产量预测不仅有助于农民提高作物品质，还有助于通过更好地决定作物收获的强度和所需的劳动来降低经营成本。 计算机视觉算法在计算果实产量时面临着各种各样的挑战，如光照方差、叶片遮挡、果实间重叠的程度以及阴影下果实的尺度变化等。为了应对这些挑战，在本研究中，我们开发了一种新的深度学习架构，在不检测对象的情况下对对象进行计数。我们的方法从整个图像的浏览中显式地估计对象的数量。这样，它减少了对象检测和定位的开销。我们的网络由多个卷积和池化层组成，除此之外还包括改进的Inception- resnet。改进后的Inception-resnet版本可以帮助我们捕捉多尺度的特征。我们的方法的框架如图1所示。 图1 我们的研究框架 本方法的主要优点是，不再需要用成千上万的带标注的真实图像去训练了，使用合成图像去训练网络并在真实图像上进行测试，就可以在真实图像上达到91%的精度了。即使在图像中存在光照不均的情况，本文的方法也表现良好。 以下是这项工作的贡献： （1）提出了一种基于卷积神经网络的果实计数的新型深度学习架构，并对Inception-resnet进行了改进。 （2）开发了一种基于模拟的学习方法，该方法用模拟数据进行训练，用真实数据进行测试。 （3）该方法对遮挡、光照变化和尺度变化都是很有效的。 （4）该算法的工作速度是1秒以内，这对于实时性要求高的应用是非常有用的。
2. 相关工作 在典型的图像分类过程中，任务是指定一个对象的存在与否，但是计算问题需要说明在场景中有多少个对象的实例。计数问题出现在一些现实世界的应用中，如显微镜图像的细胞计数[13]，空中图像的野生动物计数[14]，鱼类计数[15]，监测系统中的人群监测[16]。Kim等人提出的方法是利用一个固定的单摄像机来检测和跟踪移动的人。后来，Lempitsky等人提出了一种新的可视对象计数任务的监督学习框架，该框架基于学习过程中的中距优化损失。最近，Giuffrida等人提出了一种基于学习的方法来计算玫瑰座(模型)植物的叶子数量。他们使用一个监督的回归模型来关联基于图像的描述符，这些描述符是在无监督的情况下学习到叶子计数的。目前基于工人手工计算或花卉的产量估算的做法是非常耗时耗力的，对于大的领域是不实用的。基于机器人农业的产量自动估计提供了一种可行的解决方案。一种被广泛采用的自动产量估计方法是使用计算机视觉算法对进行计数或计算图像上的花朵密度[20,21,22,23,24,25,26]。基于计算机视觉的作物产量估计方法大致可分为两类:(1)基于区域的方法和(2)基于计数的方法。在文献中，有大量的工作处理基于区域的方法[20,21,22,23,24,25,26]。Wang等[20]为苹果园开发了一种立体摄像机自动作物产量估算系统。他们在夜间拍摄图像，以减少白天不可预知的自然光照。Li等人开发了一种基于区域语义图像分割的现场棉花检测系统。Lu等人开发了基于区域的联合作物和玉米穗分割颜色建模。尽管对基于区域的方法有广泛的关注，但对基于计数的收益估计方法的关注却很少[27,28]。Linker等[27]用彩色图像来估计在自然光照下在果园中获得的苹果数。缺点是直接光照和色彩饱和度，因为有大量的假阳性被观察到。Tabb等人开发了一种利用背景建模从视频中分割苹果的方法。 最近，基于深度学习的对象计数方法越来越流行。Segui等人与CNN探讨了计算感兴趣的概念出现次数的任务。Xie等人开发了基于卷积回归网络的显微镜细胞计数框架。Zhang等人开发了基于深度卷积神经网络的跨场景人群计数框架。French et al.[29]也在一段渔业监控视频中探索了CNN对鱼的计数。一些作者探索了/植物检测和识别的深度学习方法[30,31,32,33,34]。据我们所知，没有关于基于深度模拟学习的计数的论文;所有基于深度学习的计数方法都依赖于对象检测，然后对检测到的实例进行计数。我们的方法通过对整个图像的浏览来明确地估计对象的数量。通过这种方式，它减少了对象检测和定位的开销，并明确地学会了计数。此外，前面提到的技术依赖于大量的标记数据。在培训样本上贴标签的花费在时间和金钱上都很昂贵。在这里，我们生成合成数据，以减少为对象计数问题标记训练样本的开销。我们的方法虽然经过合成数据的训练，但在实际数据上却表现得很好。
3. 研究方法 3.1合成图像生成 深度学习需要大量的数据集，这些数据集收集和标注都非常耗时。为了解决这个问题，我们生成了合成数据来训练我们的网络，然后在真实图像上测试训练参数。合成图像如下所示，创建一个大小为128×128像素的空白图像，然后用绿色和棕色的圆形填充整个图像以模拟背景和西红柿植株，后者被高斯滤波器模糊。为了在图像中创建大小不一的西红柿，在图像上以随机的位置绘制几个随机大小的圆。生成了24000幅图像作为训练样板集，生成了2400幅图像作为测试样本集。图2展示了生成用于训练网络的合成图像的过程。合成的西红柿图像随着尺寸、比例和光照的变化而产生一定程度的重叠，以便模拟西红柿图像中可能出现的真实复杂场景。 图2 合成图像的生成
3.2卷积神经网络 CNN是最著名的深度学习方法之一；它包括不同的卷积和池化(子采样)层，类似于人类的视觉系统。通常来说，图像数据被输入到CNN的输入层，并产生与对象类相关的具有相当明显特征的向量作为输出层。在输入层和输出层之间，隐藏层以一系列卷积和池化层的形式出现，然后是全连接层。网络的训练是基于预测输出和真实标签并在前向和后向传播阶段进行的。在反向传播阶段，根据损失成本计算各参数的梯度。所有参数将根据梯度进行更新，并为下一个前向传播进行更新。经过充分的前向、后向传播的迭代之后，网络学习可以停止。
3.3Inception的体系结构 当一个卷积层试图学习同时在三维空间中使用两个空间维度和一个通道维度进行过滤时，Inception模型使这个过程变得更容易，因此从经验上看，它似乎能够以更少的参数学习更丰富的表示。Inception模型将独立研究跨通道相关性和空间相关性。Szegedy等人引入的Inception架构(Inception-v1)[37]，后来被细化为Incepiton-v2[38]、Inception-v3[39]，最近又被简化为Inception-ResNet[6]，是ImageNet数据集中表现最好的模型家族之一。受这些模式在ImageNet竞赛中的成功启发，我们在CNN网络中合并了改良版的Inception-ResNet-A。
3.4描述我们的网络架构。 本研究的神经网络设计如图3所示。网络的第一层是7×7的卷积层，紧随其后是大小为3×3，步长为2的池化层。卷积层用7×7的卷积核在输入图像的3通道（RGB）上获得64个特征图。这将压缩网络中的信息。缩小图像的维数以便减少计算时间，并且使得模型更加适应GPU的内存[41]。同样，为了降低特征图的维度，在卷积核大小为5×5的卷积层之前用了一个卷积核大小为1×1的卷积层。 图3 我们的网络架构 图像中对象的大小各不相同，因此需要一种能够在多个尺度上捕获特征的体系结构。为此，我们修改了inperception - resnet - a[6]层。为此，我们修改了Inception-ResNet-A[6]层。在正则化卷积层后面跟着两个改进的Inception-ResNet-A层，它通过连接不同内核大小的卷积层的结果来捕获不同大小的特性，以及使用跳过连接创建信息在神经网络中流动的简单路径的剩余网络[3]。该体系结构之所以被使用，是因为它在[6]上的几个竞争图像识别挑战中表现出色。在深度网络[3]中，由于残端连接加快了训练速度，所以残端网络收敛得更快。图4显示了在此工作中使用的修改后的输入- resnet模块的设计。最后1×1只卷积计算192的特性,而256年原Inception-ResNet-A[6]。 Inception-ResNet结合了Inception的思想，它通过将不同卷积核大小的卷积层的结果和残差网络的结果连接起来，从而获取不同大小的特征，而残差网络则使用跳过连接（skip connections）来创建一个简单的路径，使信息在整个神经网络中流动。 之所以用这种体系结构，是因为它在一些图像识别竞赛中表现出色[6]。由于残差连接加快了深度网络的训练，残差网络收敛速度更快。图4显示了在本研究中使用的改良的Inception-ResNet-A模块。相比于原始的Inception-ResNet-A模块中的256个特征，最终的1×1卷积仅计算出192个特征。 图4 改良的Inception-ResNet-A模块 如图4所示，改良后的Inception-ResNet-A模块由连接在一起的三个并行层级联而成。然后将这个连接的结果添加到前一层的激活中，并通过修正线性函数。在改良的Inception-ResNet-A层之后，将使用一个改良的Inception的reduction模块。如图5所示，以同时减少图像大小和增加过滤器的数量。 如图5所示，三个并行分支被连接到一个输出。这些分支包括最大池化和跨越无填充卷积(图5中的stride 2v)。模块的中间分支从输出大小192减少到128。模块的右分支分别从256、256、320和320减小到192、128和128。这些改变是为了使网络更贴近问题的复杂性。在这些修改之前，模型往往过于拟合训练数据，而在真实数据上表现得很差。 图5 改良的reduction模块 改良了reduction模块之后，又用了另一组两个Inception-ResNet-A层,接着是3×3平均池化,因为平均池化被发现用在最后的全连接层之前可以提高精度[42]。如图3所示，最终全连接层的大小为768。虽然具有大量参数的深度神经网络是非常强大的机器学习系统，但过拟合是此类网络中的一个严重问题。大型网络的使用速度也较慢，通过在测试时结合许多不同的大型神经网络的预测，很难处理过拟合。Dropout是解决这个问题的一种技巧。关键的技巧是在训练时从神经网络中随机删除神经元(以及它们的连接)。65%的连接是在网络训练时随机保留的。最后，在dropout层之后的最后一个全连接层给出输入图像中西红柿数量的预测。每次卷积后进行BN批量归一化以解决Internal Covariate Shift的问题[38]。 （译者注： 在深度网络的训练中，每一层网络的输入都会因为前一层网络参数的变化导致其分布发生改变，这就要求我们必须使用一个很小的学习率和对参数很好的初始化，但是这么做会让训练过程变得慢而且复杂。作者把这种现象称作Internal Covariate Shift。通过Batch Normalization可以很好的解决这个问题，并且文中提出的这种Batch Normalization方法前提是用mini-banch来训练神经网络。）</description>
    </item>
    
    <item>
      <title>利用CNN做图像压缩，TNG学习笔记</title>
      <link>http://senbiao.org/post/tng-cnn/</link>
      <pubDate>Fri, 30 Mar 2018 09:47:01 +0800</pubDate>
      
      <guid>http://senbiao.org/post/tng-cnn/</guid>
      <description>TNG(Tiny Network Graphics)，一种图像压缩技术，功能是将低分辨图片转化为高清版本，旨在保持图片的质量下，尽可能降低图片的大小，使用户在带宽受限的网络情况下，仍可看到高清的图像
TNG 采用的算法是卷积神经网络（CNN）。卷积神经网络是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围，适合大型图像处理。
一个卷积神经网络由卷积、池化、非线性函数、归一化层等模块组成。最终的输出根据应用而定，如在人脸识别领域，我们可以用它来提取一串特征来表示一幅人脸图片。然后通过比较特征的异同进行人脸识别。
TNG采用了量化与反量化的技术。通过量化技术将浮点数转换为整数或二进制数，这时通常采用的方法是：去除浮点数后面的小数，将浮点数变成整数。在解码端，又采用反量化技术将变换后的特征数据恢复成浮点数，如给整数加上一个随机小数。这样可以一定程度上降低量化对神经网络精度的影响，从而提高恢复图像的质量。
如何利用卷积神经网络做压缩？ 以下两张图片来自不同的文章，总体意思差不多，只有一些细节上的差异：
如图1（用深度学习进行图片压缩示意图）所示，【完整的深度学习图片压缩框架】包括CNN编码器、量化、反量化、CNN解码器、熵编码、码字估计和码率-失真优化等几个模块。编码器的作用是将图片转换为压缩特征，解码器就是从压缩特征恢复出原始图片。其中编码网络和解码器，可以用卷积、池化、非线性等模块进行设计和搭建。 如图 2（用深度学习进行图片压缩示意图）所示，完整的框架包括 CNN 编码网络、量化、反量化、CNN 解码、熵编码等几个模块。编码网络的作用是将图片转换为压缩特征，解码网络就是从压缩特征恢复出原始图片。其中编码网络和解码网络，可以用卷积、池化、非线性等模块进行设计和搭建。 如何评判压缩算法 在深入技术细节前，我们先来了解一下如何评判压缩算法。评判一个压缩算法好坏的重要指标有两个：一个是每个像素占据的比特位数（bit per pixel，BPP），一个是 PSNR。我们知道，数据在计算机中以比特形式存储，所需比特数越多则占据的存储空间越大。BPP 用于表示图像中每个像素所占据的比特数，如一张 RGB 三通道图，表示每个像素需要消耗 24 个比特。PSNR 用来评估解码后图像的恢复质量，简单理解就是 PSNR 越高，恢复质量越好。
我们举个例子，假设长宽为 768*512 的图片大小为 1M，利用深度学习技术对它编码，通过编码网络后产生包括 96*64*192 个数据单元的压缩特征数据，如果表示每个数据单元平均需要消耗 1 个比特，则编码整张图需要 96*64*192 个比特。经过压缩后，编码每个像素需要的比特数为（96*64*192）/(768*512）=3，所以 BPP 值为 3bit/pixel，压缩比为 24:3=8:1。这意味着一张 1M 的图，通过压缩后只需要消耗 0.125M 的空间，换句话说，之前只能放 1 张照片的空间，现在可以放 8 张。
如何用深度学习做压缩 谈到如何用深度学习做压缩，还是用刚才那个例子。将一张大小 768*512 的三通道图片送入编码网络，进行前向处理后，会得到占据 96*64*192 个数据单元的压缩特征。有计算机基础的读者可能会想到，这个数据单元中可放一个浮点数，整形数，或者是二进制数。那问题来了，到底应该放入什么类型的数据？从图像恢复角度和神经网络原理来讲，如果压缩特征数据都是浮点数，恢复图像质量是最高的。但一个浮点数占据 32 个比特位，那之前讲的比特数计算公式变为（96*64*192*32）/（768*512）=96，压缩后反而每个像素占据比特从 24 变到 96，非但没有压缩，反而增加了，这是一个糟糕的结果，很显然浮点数不是好的选择。
所以为了设计靠谱的算法，我们使用一种称为量化的技术，它的目的是将浮点数转换为整数或二进制数，最简单的操作是去掉浮点数后面的小数，浮点数变成整数后只占据 8 比特，则表示每个像素要占据 24 个比特位。与之对应，在解码端，可以使用反量化技术将变换后的特征数据恢复成浮点数，如给整数加上一个随机小数，这样可以一定程度上降低量化对神经网络精度的影响，从而提高恢复图像的质量。
即使压缩特征中每个数据占据 1 个比特位，可是 8:1 的压缩比在我们看来并不是一个很理想的结果。那如何进一步优化算法？再看下 BPP 的计算公式。假设每个压缩特征数据单元占据 1 个比特，则公式可写成：（96*64*192*1）/(768*512）=3，计算结果是 3 bit/pixel，从压缩的目的来看，BPP 越小越好。在这个公式中，分母由图像决定，可以调整的部分在分子，分子中 96、64、192 这三个数字与网络结构相关。很显然，当我们设计出更优的网络结构，这三个数字就会变小。</description>
    </item>
    
    <item>
      <title>常见排序算法分类</title>
      <link>http://senbiao.org/post/algorithms/</link>
      <pubDate>Wed, 21 Mar 2018 14:01:42 +0800</pubDate>
      
      <guid>http://senbiao.org/post/algorithms/</guid>
      <description> 常见排序算法一般分为以下几种：  非线性时间比较类排序：交换类排序（快速排序和冒泡排序）、插入类排序（简单插入排序和希尔排序）、选择类排序（简单选择排序和堆排序）、归并排序（二路归并排序和多路归并排序）； 线性时间非比较类排序：计数排序、基数排序和桶排序。  总结：  在比较类排序中，归并排序最快，其次是快速排序和堆排序，两者不相伯仲，但是有一点需要注意，数据初始排序状态对堆排序不会产生太大的影响，而快速排序却恰恰相反。 线性时间非比较类排序一般要优于非线性时间比较类排序，但前者对待排序元素的要求较为严格，比如计数排序要求待排序数的最大值不能太大，桶排序要求元素按照hash分桶后桶内元素的数量要均匀。线性时间非比较类排序的典型特点是以空间换时间。  </description>
    </item>
    
    <item>
      <title>机器学习涉及的数学知识</title>
      <link>http://senbiao.org/post/math-ml/</link>
      <pubDate>Mon, 19 Mar 2018 15:17:26 +0800</pubDate>
      
      <guid>http://senbiao.org/post/math-ml/</guid>
      <description>在过去几个月里，有几个人联系过我，说他们渴望进军数据科学领域，使用机器学习 (ML) 技术探索统计规律，并打造数据驱动的完美产品。但是，据我观察，一些人缺乏必要的数学直觉和框架，无法获得有用的结果。这是我决定写这篇博客文章的主要原因。最近，易用的机器学习和深度学习工具包急剧增加，比如scikit-learn、Weka、Tensorflow、R-caret等。机器学习理论是一个涵盖统计、概率、计算机科学和算法方面的领域，该理论的初衷是以迭代方式从数据中学习，找到可用于构建智能应用程序的隐藏洞察。尽管机器学习和深度学习有巨大的发展潜力，但要深入掌握算法的内部工作原理并获得良好的结果，就必须透彻地了解许多技术的数学原理。
为什么担忧数学？ 出于许多原因，机器学习的数学原理很重要，下面重点介绍部分原因： 选择正确的算法，这涉及到考虑准确率、训练时间、模型复杂性、参数数量和特征数量。 选择参数设置和验证策略。 通过理解偏差-方差权衡，识别欠拟合和过拟合。 估算正确的置信区间和不确定性。
您需要多高的数学知识水平？ 在尝试理解诸如机器学习这样的跨学科领域时，需要考虑的主要问题是，理解这些技术需要多大的数学知识量和多高的数学知识水平。此问题的答案涉及多个维度，而且取决于个人的知识水平和兴趣。对机器学习的数学公式和理论发展的研究从未间断过，一些研究人员正在研究更高级的技术。我将介绍我认为成为机器学习科学家/工程师所需的最低数学知识水平，以及每个数学概念的重要性。
 线性代数：同事 Skyler Speakman 最近说“线性代数是 21 世纪的数学”，我完全同意这种说法。在机器学习中，线性代数无处不在。要理解用于机器学习的优化方法，需要掌握许多主题，比如主成份分析 (PCA)、奇异值分解 (SVD)、矩阵特征分解、LU 分解、QR 分解/因式分解、对称矩阵、正交化/标准正交化、矩阵运算、投影、特征值和特征矢量、矢量空间，以及范数。关于线性代数，令人惊奇的是网上有如此多的资源。我总是说，由于互联网上存在大量资源，传统的课堂教学正在消亡。我最喜欢 MIT Courseware（Gilbert Strang 教授）提供的线性代数课。 概率论和统计学：机器学习与统计学并不是完全不同的领域。实际上，有人最近将机器学习定义为‘在 Mac 上实践统计学’。机器学习需要的一些基本的统计和概率理论包括组合学、概率规则和公理、贝叶斯定理、随机变量、方差和预期、条件和联合分布、标准分布（伯努利、二项式、多项式、均匀和高斯分布）、矩母函数、最大似然估计 (MLE)、先验和后验、最大后验概率估计 (MAP)，以及采样方法。 多变量微积分：一些必要的主题包括微积分、偏微分、矢量-值函数、方向梯度、海赛函数、雅可比行列式、拉普拉斯算子和拉格朗日分布。 算法和复杂优化：这对理解机器学习算法的计算效率和可伸缩性，以及利用数据集的稀疏性都很重要。需要数据结构（二叉树、哈希运算、堆、堆栈等）、动态编程、随机化和次线性算法、图表、梯度/随机下降，以及原对偶方法的知识。 其他：包括上述 4 个主要领域未涵盖的其他数学主题。这些主题包括实数和复数分析（集合和数列、拓扑、度量空间、单值和连续函数、极限、柯西核、傅里叶变换），信息论（熵、信息增益），函数空间和数集。  本文摘自：这里.</description>
    </item>
    
    <item>
      <title>K-近邻算法学习笔记</title>
      <link>http://senbiao.org/post/k-nearest-neighbors/</link>
      <pubDate>Mon, 19 Mar 2018 14:38:50 +0800</pubDate>
      
      <guid>http://senbiao.org/post/k-nearest-neighbors/</guid>
      <description>k-近邻算法，目的就是找到新数据的前k个邻居，然后根据邻居的分类来确定该数据的分类。 所谓邻居，就是距离近的。因此需要一个“距离度量”，以2个特征的样本为例，也就是在2维实数向量空间，可以使用两点距离公式计算距离：
$$ \sqrt{(x1-x2)^2+(y1-y2)^2} $$
k-近邻算法步骤如下：
计算已知类别数据集中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的k个点； 确定前k个点所在类别的出现频率； 返回前k个点所出现频率最高的类别作为当前点的预测分类。
比如，现在我这个k值取3，那么在电影例子中，按距离依次排序的三个点分别是动作片(108,5)、动作片(115,8)、爱情片(5,89)。 在这三个点中，动作片出现的频率为三分之二，爱情片出现的频率为三分之一，所以该红色圆点标记的电影为动作片。 这个判别过程就是k-近邻算法。
from：http://cuijiahua.com/blog/2017/11/ml_1_knn.html</description>
    </item>
    
    <item>
      <title>利用Tensorflow和matplotlib直观理解CNN的卷积层与池化层</title>
      <link>http://senbiao.org/post/tensorflow-cnn-pool-convolutional/</link>
      <pubDate>Tue, 16 Jan 2018 21:43:25 +0800</pubDate>
      
      <guid>http://senbiao.org/post/tensorflow-cnn-pool-convolutional/</guid>
      <description>卷积神经网络，CNN（Convolutional Neural Network），卷积神经网络是一种多层神经网络，擅长处理图像相关的深度学习问题。 与普通神经网络的区别在于，卷积神经网络包含了由卷积层（Convolutional layer）和池化层（Pooling layer）构成的特征抽取器。 本文旨在通过调用tensorflow封装好的卷积操作和池化操作函数以及python的matplotlib库，直观地感受一张图片经过卷积层和池化层之后的效果，代码如下：
#!/usr/bin/python
-- coding: UTF-8 -- import matplotlib.pyplot as plt import tensorflow as tf from PIL import Image import numpy
img = Image.open(&amp;lsquo;dog.jpg&amp;rsquo;) img_ndarray = numpy.asarray(img, dtype=&amp;lsquo;float32&amp;rsquo;) print(img_ndarray.shape) img_ndarray=img_ndarray[:,:,0] plt.figure() plt.subplot(221) plt.imshow(img_ndarray)
w=[[-1.0,-1.0,-1.0], [-1.0,9.0,-1.0], [-1.0,-1.0,-1.0]]
with tf.Session() as sess: img_ndarray=tf.reshape(img_ndarray,[1, 633, 650,1]) w=tf.reshape(w,[3,3,1,1]) img_cov=tf.nn.conv2d(img_ndarray, w, strides=[1, 1, 1, 1], padding=&amp;lsquo;SAME&amp;rsquo;) image_data=sess.run(img_cov) print(image_data.shape) plt.subplot(222) plt.imshow(image_data[0,:,:,0])
img_pool=tf.nn.max_pool(img_ndarray, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) image_data = sess.</description>
    </item>
    
    <item>
      <title>利用Tensorflow构建CNN，步骤梳理</title>
      <link>http://senbiao.org/post/tensorflow-cnn-mnist/</link>
      <pubDate>Fri, 08 Dec 2017 13:04:48 +0800</pubDate>
      
      <guid>http://senbiao.org/post/tensorflow-cnn-mnist/</guid>
      <description>利用Tensorflow构建CNN卷积神经网络，以mnist手写识别为例，步骤梳理如下：
1.定义权重函数
def weight_variable(shape): # 使用正态分布填充变量，标准差设置为0.1 initial = tf.truncated_normal(shape, stddev=0.1) #返回相应的变量 return tf.Variable(initial)  2.定义偏置函数
def bias_Variable(shape): # 定义一个常量该常量全部由0.1组成，shape由参数shape指定 initial = tf.constant(0.1, shape=shape) # 返回一个变量，变量的值由刚刚定义的常量决定 return tf.Variable(initial)  3.定义卷积层
def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)  4.定义池化层
def max_pool(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)  5.定义CNN网络结构
# 构建网络，由2个卷积层（包含激活层、池化层），一个全连接层，一个dropout层，一个softmax层 组成 def deepnn(x): x_image = tf.reshape(x, [-1, 28, 28, 1]) W_conv1 = weight_variable([5, 5, 1, 32]) b_conv1 = bias_variable([32]) h_conv1 = tf.</description>
    </item>
    
  </channel>
</rss>
