<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>深度学习 on 周森标博客</title>
    <link>http://senbiao.org/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 深度学习 on 周森标博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Mar 2018 09:47:01 +0800</lastBuildDate>
    <atom:link href="http://senbiao.org/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>利用CNN做图像压缩，TNG学习笔记</title>
      <link>http://senbiao.org/post/tng-cnn/</link>
      <pubDate>Fri, 30 Mar 2018 09:47:01 +0800</pubDate>
      
      <guid>http://senbiao.org/post/tng-cnn/</guid>
      <description>TNG(Tiny Network Graphics)，一种图像压缩技术，功能是将低分辨图片转化为高清版本，旨在保持图片的质量下，尽可能降低图片的大小，使用户在带宽受限的网络情况下，仍可看到高清的图像 TNG 采用的算法是卷积神经网络（CNN）。卷积神经网络是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围，适合大型图像处理。 一个卷积神经网络由卷积、池化、非线性函数、归一化层等模块组成。最终的输出根据应用而定，如在人脸识别领域，我们可以用它来提取一串特征来表示一幅人脸图片。然后通过比较特征的异同进行人脸识别。 TNG采用了量化与反量化的技术。通过量化技术将浮点数转换为整数或二进制数，这时通常采用的方法是：去除浮点数后面的小数，将浮点数变成整数。在解码端，又采用反量化技术将变换后的特征数据恢复成浮点数，如给整数加上一个随机小数。这样可以一定程度上降低量化对神经网络精度的影响，从而提高恢复图像的质量。 如何利用卷积神经网络做压缩？ 以下两张图片来自不同的文章，总体意思差不多，只有一些细节上的差异： 如图1（用深度学习进行图片压缩示意图）所示，【完整的深度学习图片压缩框架】包括CNN编码器、量化、反量化、CNN解码器、熵编码、码字估计和码率-失真优化等几个模块。编码器的作用是将图片转换为压缩特征，解码器就是从压缩特征恢复出原始图片。其中编码网络和解码器，可以用卷积、池化、非线性等模块进行设计和搭建。 如图 2（用深度学习进行图片压缩示意图）所示，完整的框架包括 CNN 编码网络、量化、反量化、CNN 解码、熵编码等几个模块。编码网络的作用是将图片转换为压缩特征，解码网络就是从压缩特征恢复出原始图片。其中编码网络和解码网络，可以用卷积、池化、非线性等模块进行设计和搭建。 如何评判压缩算法 在深入技术细节前，我们先来了解一下如何评判压缩算法。评判一个压缩算法好坏的重要指标有两个：一个是每个像素占据的比特位数（bit per pixel，BPP），一个是 PSNR。我们知道，数据在计算机中以比特形式存储，所需比特数越多则占据的存储空间越大。BPP 用于表示图像中每个像素所占据的比特数，如一张 RGB 三通道图，表示每个像素需要消耗 24 个比特。PSNR 用来评估解码后图像的恢复质量，简单理解就是 PSNR 越高，恢复质量越好。 我们举个例子，假设长宽为 768*512 的图片大小为 1M，利用深度学习技术对它编码，通过编码网络后产生包括 96*64*192 个数据单元的压缩特征数据，如果表示每个数据单元平均需要消耗 1 个比特，则编码整张图需要 96*64*192 个比特。经过压缩后，编码每个像素需要的比特数为（96*64*192）/(768*512）=3，所以 BPP 值为 3bit/pixel，压缩比为 24:3=8:1。这意味着一张 1M 的图，通过压缩后只需要消耗 0.125M 的空间，换句话说，之前只能放 1 张照片的空间，现在可以放 8 张。 如何用深度学习做压缩 谈到如何用深度学习做压缩，还是用刚才那个例子。将一张大小 768*512 的三通道图片送入编码网络，进行前向处理后，会得到占据 96*64*192 个数据单元的压缩特征。有计算机基础的读者可能会想到，这个数据单元中可放一个浮点数，整形数，或者是二进制数。那问题来了，到底应该放入什么类型的数据？从图像恢复角度和神经网络原理来讲，如果压缩特征数据都是浮点数，恢复图像质量是最高的。但一个浮点数占据 32 个比特位，那之前讲的比特数计算公式变为（96*64*192*32）/（768*512）=96，压缩后反而每个像素占据比特从 24 变到 96，非但没有压缩，反而增加了，这是一个糟糕的结果，很显然浮点数不是好的选择。 所以为了设计靠谱的算法，我们使用一种称为量化的技术，它的目的是将浮点数转换为整数或二进制数，最简单的操作是去掉浮点数后面的小数，浮点数变成整数后只占据 8 比特，则表示每个像素要占据 24 个比特位。与之对应，在解码端，可以使用反量化技术将变换后的特征数据恢复成浮点数，如给整数加上一个随机小数，这样可以一定程度上降低量化对神经网络精度的影响，从而提高恢复图像的质量。 即使压缩特征中每个数据占据 1 个比特位，可是 8:1</description>
    </item>
    
    <item>
      <title>利用Tensorflow和matplotlib直观理解CNN的卷积层与池化层</title>
      <link>http://senbiao.org/post/tensorflow-cnn-pool-convolutional/</link>
      <pubDate>Tue, 16 Jan 2018 21:43:25 +0800</pubDate>
      
      <guid>http://senbiao.org/post/tensorflow-cnn-pool-convolutional/</guid>
      <description>卷积神经网络，CNN（Convolutional Neural Network），卷积神经网络是一种多层神经网络，擅长处理图像相关的深度学习问题。 与普通神经网络的区别在于，卷积神经网络包含了由卷积层（Convolutional layer）和池化层（Pooling layer）构成的特征抽取器。 本文旨在通过调用tensorflow封装好的卷积操作和池化操作函数以及python的matplotlib库，直观地感受一张图片经过卷积层和池化层之后的效果，代码如下： #!/usr/bin/python -- coding: UTF-8 -- import matplotlib.pyplot as plt import tensorflow as tf from PIL import Image import numpy img = Image.open(&amp;lsquo;dog.jpg&amp;rsquo;) img_ndarray = numpy.asarray(img, dtype=&amp;lsquo;float32&amp;rsquo;) print(img_ndarray.shape) img_ndarray=img_ndarray[:,:,0] plt.figure() plt.subplot(221) plt.imshow(img_ndarray) w=[[-1.0,-1.0,-1.0], [-1.0,9.0,-1.0], [-1.0,-1.0,-1.0]] with tf.Session() as sess: img_ndarray=tf.reshape(img_ndarray,[1, 633, 650,1]) w=tf.reshape(w,[3,3,1,1]) img_cov=tf.nn.conv2d(img_ndarray, w, strides=[1, 1, 1, 1], padding=&amp;lsquo;SAME&amp;rsquo;) image_data=sess.run(img_cov) print(image_data.shape) plt.subplot(222) plt.imshow(image_data[0,:,:,0]) img_pool=tf.nn.max_pool(img_ndarray, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)</description>
    </item>
    
    <item>
      <title>利用Tensorflow构建CNN，步骤梳理</title>
      <link>http://senbiao.org/post/tensorflow-cnn-mnist/</link>
      <pubDate>Fri, 08 Dec 2017 13:04:48 +0800</pubDate>
      
      <guid>http://senbiao.org/post/tensorflow-cnn-mnist/</guid>
      <description>利用Tensorflow构建CNN卷积神经网络，以mnist手写识别为例，步骤梳理如下： 1.定义权重函数 def weight_variable(shape): # 使用正态分布填充变量，标准差设置为0.1 initial = tf.truncated_normal(shape, stddev=0.1) #返回相应的变量 return tf.Variable(initial) 2.定义偏置函数 def bias_Variable(shape): # 定义一个常量该常量全部由0.1组成，shape由参数shape指定 initial = tf.constant(0.1, shape=shape) # 返回一个变量，变量的值由刚刚定义的常量决定 return tf.Variable(initial) 3.定义卷积层 def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) 4.定义池化层 def max_pool(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) 5.定义CNN网络结构 # 构建网络，由2个卷积层（包含激活层、池化层），一个全连接层，一个dropout层，一个softmax层 组成 def deepnn(x): x_image = tf.reshape(x, [-1, 28, 28, 1]) W_conv1 = weight_variable([5, 5, 1,</description>
    </item>
    
  </channel>
</rss>
